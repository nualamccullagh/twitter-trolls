{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as N\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "import string\n",
    "%matplotlib inline\n",
    "plt.style.use('nuala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='data/stream__thelindywest___Lesdoggg___KimKardashian.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the training set data\n",
    "tweets_data = []\n",
    "tweets_file = open(filename, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the relevant info about each tweet -- this might not be exhaustive\n",
    "tweets = pd.DataFrame()\n",
    "\n",
    "# Text of the tweet\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "\n",
    "# Whether or not the tweet was retweeted\n",
    "tweets['retweeted'] = map(lambda tweet: tweet['retweeted'], tweets_data)\n",
    "\n",
    "# Username of person who tweeted it\n",
    "tweets['username'] = map(lambda tweet: tweet['user']['screen_name'], tweets_data)\n",
    "\n",
    "# Whether the account is verified\n",
    "tweets['verified'] = map(lambda tweet: tweet['user']['verified'], tweets_data)\n",
    "\n",
    "# List of hashtags used\n",
    "tweets['hashtags'] = map(lambda tweet: [tweet['entities']['hashtags'][i]['text'].lower() for i in N.arange(len(tweet['entities']['hashtags']))], tweets_data)\n",
    "\n",
    "# Number of hashtags used\n",
    "tweets['number of hashtags'] = map(lambda tweet: len(tweet['entities']['hashtags']), tweets_data)\n",
    "\n",
    "# List of users mentioned in the tweet\n",
    "tweets['user_mentions'] = map(lambda tweet: [tweet['entities']['user_mentions'][i]['screen_name'] for i in N.arange(len(tweet['entities']['user_mentions']))], tweets_data)\n",
    "\n",
    "# Whether the tweet has been favorited\n",
    "tweets['favorited'] = map(lambda tweet: tweet['favorited'], tweets_data)\n",
    "\n",
    "# Time stamp of tweet\n",
    "tweets['timestamp'] = map(lambda tweet: int(tweet['created_at'].split()[3].split(':')[0])+int(tweet['created_at'].split()[3].split(':')[1])/60., tweets_data)\n",
    "\n",
    "# Source of tweet (iPhone, Android, etc)\n",
    "tweets['source'] = map(lambda tweet: tweet['source'], tweets_data)\n",
    "\n",
    "# Language\n",
    "tweets['language'] = map(lambda tweet: tweet['lang'], tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change text to lower case\n",
    "tweets['text']=tweets['text'].str.lower()\n",
    "\n",
    "# Replace URLs with 'httpaddr'\n",
    "tweets['text'] = tweets['text'].str.replace('https?:.*', 'httpaddr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new column called 'Source Category'\n",
    "tweets['source category'] = 0\n",
    "tweets.loc[tweets['source'].str.contains('iPhone|iPad'), 'source category'] = 1\n",
    "tweets.loc[tweets['source'].str.contains('Android'), 'source category'] = 2\n",
    "tweets.loc[tweets['source'].str.contains('Web Client'), 'source category'] = 3\n",
    "tweets.loc[tweets['source'].str.contains('twittbot.net'), 'source category'] = 4\n",
    "tweets.loc[tweets['source'].str.contains('SocialFlow'), 'source category'] = 5\n",
    "tweets.loc[tweets['source'].str.contains('Windows Phone'), 'source category'] = 6\n",
    "tweets.loc[tweets['source'].str.contains('BlackBerry'), 'source category'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set classification (as found in the other notebook)\n",
    "troll_class = N.load('/Users/nuala/Documents/Research/Code/repos/twitter-trolls/data/training_set_classification.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of words in each tweet - this will be used to figure out the most common words used\n",
    "text_lists = tweets.loc[:, 'text'].str.split('[\\s,\\.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data sets of positive / negative / curse words, these may come in handy\n",
    "negative_words = N.loadtxt('data/opinion-lexicon-English/negative-words.txt', dtype=str, comments=';').tolist()\n",
    "positive_words = N.loadtxt('data/opinion-lexicon-English/positive-words.txt', dtype=str, comments=';').tolist()\n",
    "curse_words1 = N.loadtxt('data/opinion-lexicon-English/curse-words.txt', dtype=str, delimiter='\\n').tolist()\n",
    "curse_words2 = N.loadtxt('data/opinion-lexicon-English/swearWords.txt', dtype=str, delimiter='\\n').tolist()\n",
    "really_bad_words = N.loadtxt('data/opinion-lexicon-English/really_bad_words.txt', dtype =str, delimiter='\\n').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A list of common words we don't care about\n",
    "common_words = ['and', 'a','an', 'the', 'is', 'are', 'in', 'of', 'to', 'this', 'that', 'it', 'its', 'on', 'at', 'as']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get 2 lists of all the (interesting) words used by trolls and non-trolls\n",
    "\n",
    "troll_text = []\n",
    "normal_text = []\n",
    "\n",
    "for i in text_lists.index:\n",
    "    # Only consider tweets that are in English\n",
    "    if (tweets.loc[i, 'language']!='en'):\n",
    "        continue\n",
    "        \n",
    "    # If this is a troll tweet, add text to troll_text, otherwise add it to normal_text\n",
    "    if (troll_class[i]==True):\n",
    "        wordlist = troll_text\n",
    "    else:\n",
    "        wordlist = normal_text\n",
    "        \n",
    "    # Go through the words in the tweet\n",
    "    for word in text_lists[i]:\n",
    "        tmp = word.encode('ascii', 'ignore')\n",
    "        # we don't care about mentions or hash tags or RT or URLs\n",
    "        if ((tmp.startswith('@')) | (tmp.startswith('#')) | (tmp=='rt') | (tmp.startswith('htt'))):\n",
    "            continue\n",
    "        # take out punctuation\n",
    "        tmp = tmp.translate(None, string.punctuation)\n",
    "        if (tmp == ''):\n",
    "            continue\n",
    "        # don't include words that are just numbers\n",
    "        if (re.search('[a-z]', tmp) == None):\n",
    "            continue\n",
    "        # don't include the common words\n",
    "        if (tmp in common_words):\n",
    "            continue\n",
    "        if (tmp.startswith('kim')):\n",
    "            continue\n",
    "        # if it makes it here, we probably have a proper word\n",
    "        wordlist.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words added to non-troll word list: 9437\n",
      "Number of words added to troll word list: 605\n"
     ]
    }
   ],
   "source": [
    "print 'Number of words added to non-troll word list:', len(normal_text)\n",
    "print 'Number of words added to troll word list:', len(troll_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get unique words in these lists, sort by most common\n",
    "utext1, count1 = N.unique(normal_text, return_counts=True)\n",
    "utext2, count2 = N.unique(troll_text, return_counts=True)\n",
    "index1 = N.argsort(count1)[::-1]\n",
    "index2 = N.argsort(count2)[::-1]\n",
    "\n",
    "# the following are a list of the words most commonly used by things classified as trolls and non-trolls\n",
    "# it is in order of the most common\n",
    "nontroll_words = utext1[index1]\n",
    "troll_words = utext2[index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common troll words: ['you' 'i' 'god' 'u' 'like' 'so' 'ya' 'shit' 'me' 'what' 'for' 'know' 'get'\n",
      " 'amp' 'my' 'was' 'your' 'got' 'good' 'ur' 'do' 'make' 'her' 'lol' 'just'\n",
      " 'robbery' 'all' 'fucking' 'whore' 'black']\n",
      "Most common non-troll words: ['about' 'report' 'much' 'online' 'petition' 'ado' 'you' 'i' 'be' 'robbery'\n",
      " 'for' 'what' 'her' 'your' 'do' 'she' 'so' 'was' 'shes' 'like' 'people'\n",
      " 'new' 'not' 'robbed' 'my' 'but' 'amp' 'or' 'im' 'paris']\n"
     ]
    }
   ],
   "source": [
    "print 'Most common troll words:', troll_words[0:30]\n",
    "print 'Most common non-troll words:', nontroll_words[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the 50 most common troll words and 50 most common normal words\n",
    "num = 50\n",
    "word_list = troll_words[0:num].tolist() + nontroll_words[0:num].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize new columns\n",
    "for i in N.arange(len(word_list)):\n",
    "    column_name = 'used word %i'%i\n",
    "    tweets[column_name]=False\n",
    "for i in N.arange(len(really_bad_words)):\n",
    "    column_name = 'bad word %i'%i\n",
    "    tweets[column_name]=False\n",
    "    \n",
    "\n",
    "for i in N.arange(len(tweets)):\n",
    "    for word in text_lists[i]:\n",
    "        tmp = word.encode('ascii', 'ignore')\n",
    "        #first check the list of actual words in the tweet (not hashtags)\n",
    "        if (word in word_list):\n",
    "            ii = word_list.index(word)\n",
    "            column_name = 'used word %i'%ii\n",
    "            tweets.loc[i, column_name]=True\n",
    "        if (word in really_bad_words):\n",
    "            ii = really_bad_words.index(word)\n",
    "            column_name = 'bad word %i'%ii\n",
    "            tweets.loc[i, column_name]=True\n",
    "        else:\n",
    "            continue\n",
    "    #next check the list of hashtags\n",
    "    for word in tweets.loc[i, 'hashtags']:\n",
    "        tmp = word.encode('ascii', 'ignore')\n",
    "        for bad_word in really_bad_words:\n",
    "            if (word.find(bad_word)>-1):\n",
    "                ii = really_bad_words.index(bad_word)\n",
    "                column_name = 'bad word %i'%ii\n",
    "                tweets.loc[i, column_name]=True\n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'text', u'retweeted', u'username', u'verified', u'hashtags',\n",
       "       u'number of hashtags', u'user_mentions', u'favorited', u'timestamp',\n",
       "       u'source',\n",
       "       ...\n",
       "       u'used word 99', u'bad word 0', u'bad word 1', u'bad word 2',\n",
       "       u'bad word 3', u'bad word 4', u'bad word 5', u'bad word 6',\n",
       "       u'bad word 7', u'source category'],\n",
       "      dtype='object', length=120)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features to include:\n",
    "# *whether the tweet contains the N most common troll words\n",
    "# *whether the tweet contains a few really bad curse words (these might not get pulled out as separate words\n",
    "#  because they could be in a hashtag, so just check if the full text of the tweet contains those words)\n",
    "# *number of hash tags\n",
    "# *device used (iphone/ipad, android, blackberry, etc)\n",
    "# *time stamp?\n",
    "# *verified\n",
    "\n",
    "columns = ['verified', 'timestamp', 'number of hashtags', 'source category']\n",
    "\n",
    "for i in N.arange(len(word_list)):\n",
    "    column_name = 'used word %i'%i\n",
    "    columns.append(column_name)\n",
    "for i in N.arange(len(really_bad_words)):\n",
    "    column_name = 'bad word %i'%i\n",
    "    columns.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = tweets.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntot = len(features)\n",
    "ntrain = N.int(ntot / 3)\n",
    "ncross = N.int(ntot / 3)\n",
    "ntest = ntot - ntrain - ncross\n",
    "\n",
    "# Randomize indices\n",
    "rind = N.random.permutation(ntot)\n",
    "\n",
    "# training set\n",
    "training_set = features.loc[rind[0:ntrain], :]\n",
    "ytrain = troll_class[rind[0:ntrain]]\n",
    "\n",
    "# cross validation set\n",
    "validation_set = features.loc[rind[ntrain:ntrain+ncross], :]\n",
    "yval = troll_class[rind[ntrain:ntrain+ncross]]\n",
    "\n",
    "# test set\n",
    "test_set = features.loc[rind[ntrain+ncross:], :]\n",
    "ytest = troll_class[rind[ntrain+ncross:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 507\n",
      "Size of validation set: 507\n",
      "Size of test set: 509\n"
     ]
    }
   ],
   "source": [
    "print 'Size of training set:', len(training_set)\n",
    "print 'Size of validation set:', len(validation_set)\n",
    "print 'Size of test set:', len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Support vector machine\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the values of C and kernel that minimize false positives (wrongly classify non-trolling tweets as trolling)\n",
    "numc = 20\n",
    "nkernel = 3\n",
    "\n",
    "precision = N.zeros((numc, nkernel), dtype=N.float)\n",
    "\n",
    "j = 0\n",
    "for kernel in ['linear', 'rbf', 'poly']:\n",
    "    i = 0\n",
    "    for c in N.logspace(-3, 1, numc):\n",
    "        clf = svm.SVC(kernel=kernel, C = c)\n",
    "        clf.fit(training_set, ytrain)\n",
    "        ypredict = clf.predict(validation_set)\n",
    "        \n",
    "        precision[i,j] = N.sum((ypredict == True) & (yval == True))/ N.float(N.sum(yval==True))\n",
    "        i = i+1\n",
    "    j = j+1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value of C: 1.43844988829\n",
      "Best kernel: linear\n",
      "False positive rate: 0.673913043478\n"
     ]
    }
   ],
   "source": [
    "ind = N.unravel_index(precision.argmax(), precision.shape)\n",
    "cval = N.logspace(-3, 1, numc)[ind[0]]\n",
    "kernel = ['linear', 'rbf', 'poly'][ind[1]]\n",
    "\n",
    "print 'Best value of C:', cval\n",
    "print 'Best kernel:', kernel\n",
    "print 'False positive rate:', N.max(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.4384498882876631, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using optimal value of C and kernel, train the algorithm again\n",
    "clf = svm.SVC(kernel=kernel, C=cval)\n",
    "clf.fit(training_set, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypredict = clf.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90766208251473479"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.sum(ypredict == ytest) / N.float(len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False negative rate: 0.0648330058939\n"
     ]
    }
   ],
   "source": [
    "print 'False negative rate:', N.sum((ypredict == False) & (ytest == True))/ N.float(len(ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate: 0.0275049115914\n"
     ]
    }
   ],
   "source": [
    "print 'False positive rate:', N.sum((ypredict == True) & (ytest == False))/ N.float(len(ytest))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
