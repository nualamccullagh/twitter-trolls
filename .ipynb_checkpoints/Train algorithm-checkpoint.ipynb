{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as N\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "import string\n",
    "plt.style.use('nuala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='data/stream__thelindywest___Lesdoggg___KimKardashian.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the training set data\n",
    "tweets_data = []\n",
    "tweets_file = open(filename, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the relevant info about each tweet -- this might not be exhaustive\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "tweets['retweeted'] = map(lambda tweet: tweet['retweeted'], tweets_data)\n",
    "tweets['username'] = map(lambda tweet: tweet['user']['screen_name'], tweets_data)\n",
    "tweets['verified'] = map(lambda tweet: int(tweet['user']['verified']), tweets_data)\n",
    "tweets['hashtags'] = map(lambda tweet: [xx['text'].lower() for xx in tweet['entities']['hashtags']], tweets_data)\n",
    "tweets['number of hashtags'] = map(lambda tweet: len(tweet['entities']['hashtags']), tweets_data)\n",
    "tweets['user_mentions'] = map(lambda tweet: [tweet['entities']['user_mentions'][i]['screen_name'] for i in N.arange(len(tweet['entities']['user_mentions']))], tweets_data)\n",
    "\n",
    "tweets['favorited'] = map(lambda tweet: tweet['favorited'], tweets_data)\n",
    "tweets['timestamp'] = map(lambda tweet: int(tweet['created_at'].split()[3].split(':')[0])+int(tweet['created_at'].split()[3].split(':')[1])/60., tweets_data)\n",
    "tweets['source'] = map(lambda tweet: tweet['source'], tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change text to lower case\n",
    "tweets['text']=tweets['text'].str.lower()\n",
    "tweets['text'] = tweets['text'].str.replace('https?:.*', 'httpaddr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training set classification (as found in the other notebook)\n",
    "troll_class = N.load('/Users/nuala/Documents/Research/Code/repos/twitter-trolls/data/training_set_classification.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list of words in each tweet - this will be used to figure out the most common words used\n",
    "\n",
    "\n",
    "text_lists = tweets['text'].str.split('[\\s,\\.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data sets of positive / negative / curse words, these may come in handy\n",
    "negative_words = N.loadtxt('data/opinion-lexicon-English/negative-words.txt', dtype=str, comments=';').tolist()\n",
    "positive_words = N.loadtxt('data/opinion-lexicon-English/positive-words.txt', dtype=str, comments=';').tolist()\n",
    "curse_words1 = N.loadtxt('data/opinion-lexicon-English/curse-words.txt', dtype=str, delimiter='\\n').tolist()\n",
    "curse_words2 = N.loadtxt('data/opinion-lexicon-English/swearWords.txt', dtype=str, delimiter='\\n').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a list of common words we don't care about\n",
    "\n",
    "common_words = ['and', 'a','an', 'the', 'is', 'are', 'in', 'of', 'to', 'this', 'that', 'it', 'its', 'on', 'at', 'as']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get 2 lists of all the (interesting) words used by trolls and non-trolls\n",
    "\n",
    "troll_text = []\n",
    "normal_text = []\n",
    "\n",
    "for i in N.arange(len(text_lists)):\n",
    "    if (troll_class[i]==True):\n",
    "        wordlist = troll_text\n",
    "    else:\n",
    "        wordlist = normal_text\n",
    "    for word in text_lists[i]:\n",
    "            \n",
    "        tmp = word.encode('ascii', 'ignore')\n",
    "        # we don't care about mentions or hash tags or RT or URLs\n",
    "        if ((tmp.startswith('@')) | (tmp.startswith('#')) | (tmp=='rt') | (tmp.startswith('htt'))):\n",
    "            continue\n",
    "        # take out punctuation\n",
    "        tmp = tmp.translate(None, string.punctuation)\n",
    "        if (tmp == ''):\n",
    "            continue\n",
    "        # don't include words that are just numbers\n",
    "        if (re.search('[a-z]', tmp) == None):\n",
    "            continue\n",
    "        # don't include the common words\n",
    "        if (tmp in common_words):\n",
    "            continue\n",
    "        if (tmp.startswith('kim')):\n",
    "            continue\n",
    "        # if it makes it here, we probably have a proper word\n",
    "        wordlist.append(tmp)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10798\n",
      "613\n"
     ]
    }
   ],
   "source": [
    "print len(normal_text)\n",
    "print len(troll_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utext1, count1 = N.unique(normal_text, return_counts=True)\n",
    "utext2, count2 = N.unique(troll_text, return_counts=True)\n",
    "index1 = N.argsort(count1)[::-1]\n",
    "index2 = N.argsort(count2)[::-1]\n",
    "\n",
    "# the following are a list of the words most commonly used by things classified as trolls and non-trolls\n",
    "# it is in order of the most common\n",
    "nontroll_words = utext1[index1]\n",
    "troll_words = utext2[index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you' 'i' 'god' 'so' 'u' 'like' 'ya' 'lol' 'shit' 'what' 'me' 'amp' 'was'\n",
      " 'get' 'slayer' 'your' 'my' 'know' 'for' 'fucking' 'got' 'good' 'do' 'make'\n",
      " 'just' 'whore' 'her' 'all' 'ur' 'slut']\n",
      "['about' 'report' 'much' 'petition' 'online' 'ado' 'you' 'i' 'be' 'de'\n",
      " 'robbery' 'for' 'do' 'des' 'what' 'your' 'her' 'she' 'en' 'so' 'like'\n",
      " 'was' 'shes' 'people' 'new' 'not' 'robbed' 'my' 'amp' 'paris']\n"
     ]
    }
   ],
   "source": [
    "print troll_words[0:30]\n",
    "print nontroll_words[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['source category'] = 0\n",
    "tweets.loc[tweets['source'].str.contains('iPhone|iPad'), 'source category'] = 1\n",
    "tweets.loc[tweets['source'].str.contains('Android'), 'source category'] = 2\n",
    "tweets.loc[tweets['source'].str.contains('Web Client'), 'source category'] = 3\n",
    "tweets.loc[tweets['source'].str.contains('twittbot.net'), 'source category'] = 4\n",
    "tweets.loc[tweets['source'].str.contains('SocialFlow'), 'source category'] = 5\n",
    "tweets.loc[tweets['source'].str.contains('Windows Phone'), 'source category'] = 6\n",
    "tweets.loc[tweets['source'].str.contains('BlackBerry'), 'source category'] = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num = 50\n",
    "word_list = troll_words[0:num].tolist()\n",
    "\n",
    "#check for these words as well, these are the only ones that will be checked for in the hashtags\n",
    "really_bad_words = ['cunt', 'nigger', 'slut']\n",
    "\n",
    "\n",
    "for i in N.arange(len(word_list)):\n",
    "    column_name = 'used word %i'%i\n",
    "    tweets[column_name]=0\n",
    "for i in N.arange(len(really_bad_words)):\n",
    "    column_name = 'bad word %i'%i\n",
    "    tweets[column_name]=0\n",
    "    \n",
    "for i in N.arange(len(tweets)):\n",
    "    for word in text_lists[i]:\n",
    "        tmp = word.encode('ascii', 'ignore')\n",
    "        #first check the list of actual words in the tweet (not hashtags)\n",
    "        if (word in word_list):\n",
    "            ii = word_list.index(word)\n",
    "            column_name = 'used word %i'%ii\n",
    "            tweets.loc[i, column_name]=1\n",
    "        if (word in really_bad_words):\n",
    "            ii = really_bad_words.index(word)\n",
    "            column_name = 'bad word %i'%ii\n",
    "            tweets.loc[i, column_name]=1\n",
    "        else:\n",
    "            continue\n",
    "    #next check the list of hashtags\n",
    "    for word in tweets.loc[i, 'hashtags']:\n",
    "        tmp = word.encode('ascii', 'ignore')\n",
    "        \n",
    "        for bad_word in really_bad_words:\n",
    "            if (word.find(bad_word)>-1):\n",
    "                ii = really_bad_words.index(bad_word)\n",
    "                column_name = 'bad word %i'%ii\n",
    "                tweets.loc[i, column_name]=1\n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1288    InfamousSOB247\n",
       "Name: username, dtype: object"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[tweets['bad word 0']==1, 'username']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features to include:\n",
    "# *whether the tweet contains the N most common troll words\n",
    "# *whether the tweet contains a few really bad curse words (these might not get pulled out as separate words\n",
    "#  because they could be in a hashtag, so just check if the full text of the tweet contains those words)\n",
    "# *number of hash tags\n",
    "# *device used (iphone/ipad, android, blackberry, etc)\n",
    "# *time stamp?\n",
    "# *verified\n",
    "\n",
    "columns = ['verified', 'timestamp', 'number of hashtags', 'source category']\n",
    "\n",
    "for i in N.arange(len(word_list)):\n",
    "    column_name = 'used word %i'%i\n",
    "    columns.append(column_name)\n",
    "for i in N.arange(len(really_bad_words)):\n",
    "    column_name = 'bad word %i'%i\n",
    "    columns.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = tweets.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntot = len(features)\n",
    "ntrain = N.int(ntot / 2)\n",
    "ncross = N.int(ntot / 2)\n",
    "rind = N.random.permutation(ntot)\n",
    "training_set = features.loc[rind[0:ntrain], :]\n",
    "ytrain = troll_class[rind[0:ntrain]]\n",
    "validation_set = features.loc[rind[ntrain:], :]\n",
    "yval = troll_class[rind[ntrain:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761\n",
      "761\n",
      "762\n"
     ]
    }
   ],
   "source": [
    "print len(training_set)\n",
    "print len(ytrain)\n",
    "print len(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(training_set, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypredict = clf.predict(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92125984251968507"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.sum(ypredict == yval) / N.float(len(yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([105,  85], dtype=int32)"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
