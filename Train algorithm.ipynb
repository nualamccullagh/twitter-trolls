{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as N\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "import string\n",
    "plt.style.use('nuala')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='data/stream__thelindywest___Lesdoggg___KimKardashian.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the training set data\n",
    "tweets_data = []\n",
    "tweets_file = open(filename, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the relevant info about each tweet -- this might not be exhaustive\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "tweets['retweeted'] = map(lambda tweet: tweet['retweeted'], tweets_data)\n",
    "tweets['username'] = map(lambda tweet: tweet['user']['screen_name'], tweets_data)\n",
    "tweets['verified'] = map(lambda tweet: tweet['user']['verified'], tweets_data)\n",
    "tweets['hashtags'] = map(lambda tweet: tweet['entities']['hashtags'], tweets_data)\n",
    "tweets['number of hashtags'] = map(lambda tweet: len(tweet['entities']['hashtags']), tweets_data)\n",
    "tweets['user_mentions'] = map(lambda tweet: [tweet['entities']['user_mentions'][i]['screen_name'] for i in N.arange(len(tweet['entities']['user_mentions']))], tweets_data)\n",
    "\n",
    "tweets['favorited'] = map(lambda tweet: tweet['favorited'], tweets_data)\n",
    "tweets['timestamp'] = map(lambda tweet: tweet['created_at'], tweets_data)\n",
    "tweets['source'] = map(lambda tweet: tweet['source'], tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change text to lower case\n",
    "tweets['text']=tweets['text'].str.lower()\n",
    "tweets['text'] = tweets['text'].str.replace('https?:.*', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training set classification (as found in the other notebook)\n",
    "troll_class = N.load('/Users/nuala/Documents/Research/Code/repos/twitter-trolls/data/training_set_classification.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list of words in each tweet - this will be used to figure out the most common words used\n",
    "\n",
    "\n",
    "text_lists = tweets['text'].str.split('[\\s,\\.]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data sets of positive / negative / curse words, these may come in handy\n",
    "negative_words = N.loadtxt('data/opinion-lexicon-English/negative-words.txt', dtype=str, comments=';').tolist()\n",
    "positive_words = N.loadtxt('data/opinion-lexicon-English/positive-words.txt', dtype=str, comments=';').tolist()\n",
    "curse_words1 = N.loadtxt('data/opinion-lexicon-English/curse-words.txt', dtype=str, delimiter='\\n').tolist()\n",
    "curse_words2 = N.loadtxt('data/opinion-lexicon-English/swearWords.txt', dtype=str, delimiter='\\n').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a list of common words we don't care about\n",
    "\n",
    "common_words = ['and', 'a','an', 'the', 'is', 'are', 'in', 'of', 'to', 'this', 'that', 'it', 'its', 'on', 'at', 'as']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get 2 lists of all the (interesting) words used by trolls and non-trolls\n",
    "\n",
    "troll_text = []\n",
    "normal_text = []\n",
    "\n",
    "for i in N.arange(len(text_lists)):\n",
    "    if (troll_class[i]==True):\n",
    "        wordlist = troll_text\n",
    "    else:\n",
    "        wordlist = normal_text\n",
    "    for word in text_lists[i]:\n",
    "            \n",
    "        tmp = word.encode('ascii', 'ignore')\n",
    "        # we don't care about mentions or hash tags or RT or URLs\n",
    "        if ((tmp.startswith('@')) | (tmp.startswith('#')) | (tmp=='rt') | (tmp.startswith('htt'))):\n",
    "            continue\n",
    "        # take out punctuation\n",
    "        tmp = tmp.translate(None, string.punctuation)\n",
    "        if (tmp == ''):\n",
    "            continue\n",
    "        # don't include words that are just numbers\n",
    "        if (re.search('[a-z]', tmp) == None):\n",
    "            continue\n",
    "        # don't include the common words\n",
    "        if (tmp in common_words):\n",
    "            continue\n",
    "        # if it makes it here, we probably have a proper word\n",
    "        wordlist.append(tmp)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10828\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "print len(normal_text)\n",
    "print len(troll_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utext1, count1 = N.unique(normal_text, return_counts=True)\n",
    "utext2, count2 = N.unique(troll_text, return_counts=True)\n",
    "index1 = N.argsort(count1)[::-1]\n",
    "index2 = N.argsort(count2)[::-1]\n",
    "\n",
    "# the following are a list of the words most commonly used by things classified as trolls and non-trolls\n",
    "# it is in order of the most common\n",
    "nontroll_words = utext1[index1]\n",
    "troll_words = utext2[index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you' 'i' 'kim' 'god' 'u' 'so' 'like' 'ya' 'lol' 'what' 'kimisaripoff'\n",
      " 'shit' 'me' 'your' 'my' 'know' 'amp' 'slayer' 'for' 'get' 'was' 'just'\n",
      " 'fucking' 'ur' 'got' 'her' 'good' 'whore' 'all' 'do']\n",
      "['about' 'report' 'much' 'petition' 'online' 'ado' 'you' 'i' 'be' 'de'\n",
      " 'robbery' 'for' 'do' 'des' 'what' 'your' 'her' 'she' 'en' 'like' 'was'\n",
      " 'so' 'shes' 'people' 'new' 'not' 'robbed' 'my' 'amp' 'paris']\n"
     ]
    }
   ],
   "source": [
    "print troll_words[0:30]\n",
    "print nontroll_words[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['source category'] = 0\n",
    "tweets.loc[tweets['source'].str.contains('iPhone|iPad'), 'source category'] = 1\n",
    "tweets.loc[tweets['source'].str.contains('Android'), 'source category'] = 2\n",
    "tweets.loc[tweets['source'].str.contains('Web Client'), 'source category'] = 3\n",
    "tweets.loc[tweets['source'].str.contains('twittbot.net'), 'source category'] = 4\n",
    "tweets.loc[tweets['source'].str.contains('SocialFlow'), 'source category'] = 5\n",
    "tweets.loc[tweets['source'].str.contains('Windows Phone'), 'source category'] = 6\n",
    "tweets.loc[tweets['source'].str.contains('BlackBerry'), 'source category'] = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26      Wed Oct 05 23:17:40 +0000 2016\n",
       "27      Wed Oct 05 23:17:44 +0000 2016\n",
       "29      Wed Oct 05 23:17:49 +0000 2016\n",
       "30      Wed Oct 05 23:17:53 +0000 2016\n",
       "33      Wed Oct 05 23:18:06 +0000 2016\n",
       "34      Wed Oct 05 23:18:09 +0000 2016\n",
       "36      Wed Oct 05 23:18:13 +0000 2016\n",
       "37      Wed Oct 05 23:18:16 +0000 2016\n",
       "39      Wed Oct 05 23:18:20 +0000 2016\n",
       "41      Wed Oct 05 23:18:25 +0000 2016\n",
       "42      Wed Oct 05 23:18:28 +0000 2016\n",
       "43      Wed Oct 05 23:18:33 +0000 2016\n",
       "45      Wed Oct 05 23:18:38 +0000 2016\n",
       "49      Wed Oct 05 23:18:43 +0000 2016\n",
       "51      Wed Oct 05 23:18:47 +0000 2016\n",
       "52      Wed Oct 05 23:18:51 +0000 2016\n",
       "54      Wed Oct 05 23:18:54 +0000 2016\n",
       "56      Wed Oct 05 23:18:56 +0000 2016\n",
       "57      Wed Oct 05 23:18:57 +0000 2016\n",
       "59      Wed Oct 05 23:19:10 +0000 2016\n",
       "61      Wed Oct 05 23:19:18 +0000 2016\n",
       "63      Wed Oct 05 23:19:26 +0000 2016\n",
       "67      Wed Oct 05 23:19:33 +0000 2016\n",
       "71      Wed Oct 05 23:19:41 +0000 2016\n",
       "74      Wed Oct 05 23:19:29 +0000 2016\n",
       "84      Wed Oct 05 23:20:35 +0000 2016\n",
       "85      Wed Oct 05 23:20:38 +0000 2016\n",
       "87      Wed Oct 05 23:20:41 +0000 2016\n",
       "90      Wed Oct 05 23:20:55 +0000 2016\n",
       "190     Wed Oct 05 23:32:51 +0000 2016\n",
       "                     ...              \n",
       "1386    Thu Oct 06 01:18:42 +0000 2016\n",
       "1387    Thu Oct 06 01:18:44 +0000 2016\n",
       "1389    Thu Oct 06 01:18:47 +0000 2016\n",
       "1390    Thu Oct 06 01:18:49 +0000 2016\n",
       "1400    Thu Oct 06 01:19:41 +0000 2016\n",
       "1403    Thu Oct 06 01:19:53 +0000 2016\n",
       "1406    Thu Oct 06 01:19:58 +0000 2016\n",
       "1407    Thu Oct 06 01:20:00 +0000 2016\n",
       "1408    Thu Oct 06 01:20:02 +0000 2016\n",
       "1409    Thu Oct 06 01:20:05 +0000 2016\n",
       "1411    Thu Oct 06 01:20:07 +0000 2016\n",
       "1424    Thu Oct 06 01:21:01 +0000 2016\n",
       "1428    Thu Oct 06 01:21:21 +0000 2016\n",
       "1431    Thu Oct 06 01:21:30 +0000 2016\n",
       "1432    Thu Oct 06 01:21:38 +0000 2016\n",
       "1433    Thu Oct 06 01:21:40 +0000 2016\n",
       "1435    Thu Oct 06 01:21:43 +0000 2016\n",
       "1438    Thu Oct 06 01:21:49 +0000 2016\n",
       "1441    Thu Oct 06 01:21:52 +0000 2016\n",
       "1443    Thu Oct 06 01:21:54 +0000 2016\n",
       "1445    Thu Oct 06 01:22:02 +0000 2016\n",
       "1446    Thu Oct 06 01:22:03 +0000 2016\n",
       "1448    Thu Oct 06 01:22:06 +0000 2016\n",
       "1449    Thu Oct 06 01:22:10 +0000 2016\n",
       "1451    Thu Oct 06 01:22:13 +0000 2016\n",
       "1452    Thu Oct 06 01:22:16 +0000 2016\n",
       "1456    Thu Oct 06 01:22:26 +0000 2016\n",
       "1464    Thu Oct 06 01:22:59 +0000 2016\n",
       "1468    Thu Oct 06 01:23:05 +0000 2016\n",
       "1469    Thu Oct 06 01:23:09 +0000 2016\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.loc[troll_class == 1, 'timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'<a href=\"http://twittbot.net/\" rel=\"nofollow\">twittbot.net</a>'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['source'][tweets['source category']==0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4        True\n",
       "5       False\n",
       "6       False\n",
       "7       False\n",
       "8       False\n",
       "9        True\n",
       "10       True\n",
       "11       True\n",
       "12      False\n",
       "13      False\n",
       "14      False\n",
       "15      False\n",
       "16      False\n",
       "17       True\n",
       "18       True\n",
       "19      False\n",
       "20      False\n",
       "21      False\n",
       "22      False\n",
       "23      False\n",
       "24      False\n",
       "25      False\n",
       "26      False\n",
       "27      False\n",
       "28      False\n",
       "29      False\n",
       "        ...  \n",
       "1493     True\n",
       "1494    False\n",
       "1495     True\n",
       "1496    False\n",
       "1497    False\n",
       "1498    False\n",
       "1499     True\n",
       "1500    False\n",
       "1501    False\n",
       "1502    False\n",
       "1503    False\n",
       "1504    False\n",
       "1505     True\n",
       "1506    False\n",
       "1507     True\n",
       "1508    False\n",
       "1509     True\n",
       "1510    False\n",
       "1511    False\n",
       "1512    False\n",
       "1513    False\n",
       "1514    False\n",
       "1515    False\n",
       "1516     True\n",
       "1517    False\n",
       "1518    False\n",
       "1519    False\n",
       "1520    False\n",
       "1521    False\n",
       "1522    False\n",
       "Name: source, dtype: bool"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features to include:\n",
    "# *whether the tweet contains the N most common troll words\n",
    "# *whether the tweet contains a few really bad curse words (these might not get pulled out as separate words\n",
    "#  because they could be in a hashtag, so just check if the full text of the tweet contains those words)\n",
    "# *number of hash tags\n",
    "# *device used (iphone/ipad, android, blackberry, etc)\n",
    "# *time stamp?\n",
    "# *verified\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
